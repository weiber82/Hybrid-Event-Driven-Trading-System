文獻導讀
Burton, J., Al Moubayed, N., & Enshaei, A. (2023).
Natural Language Explanations for Machine Learning Classification Decisions.
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10191637


1. 研究背景與動機

大多數可解釋人工智慧（XAI）方法，如 LIME、SHAP、Integrated Gradients（IG）、Layer-wise Relevance Propagation（LRP），皆能產生「特徵的重要度」數值或視覺化圖表。然而：

專家之外的使用者仍很難理解這些數值與圖形

圖表無法直接說明「模型為什麼做出這樣的預測」

現有 XAI 常停留在數值層級，缺乏自然語言敘述

因此，本研究提出一種新的任務：

將模型的數值特徵重要度 → 轉換成自然語言敘述
讓使用者能用閱讀文章的方式了解模型的判斷原因。

2. 研究目的

本研究的主要目標為：

建立「數值解釋 → 自然語言解釋」的新型資料集 TEXEN

使用深度 NLP 模型（T5、BART）生成自然語言解釋

評估模型生成的語句是否流暢、正確且可理解

分析資料增強是否能改善模型表現

探討此方法是否適合協助增強模型可解釋性

3. 研究方法與系統架構

整體流程由兩大部分組成：

3.1 數值解釋流程（Numerical Explanation Pipeline）

包含：

模型預測（Classifier）：輸出各類別機率

XAI 解釋器（Explainer）：LIME、SHAP、IG、LRP

輸出：每個特徵的正向／負向貢獻與強度

此流程等同於金融模型中使用 SHAP 的步驟。

3.2 文本解釋流程（Textual Explanation Pipeline）

本研究核心貢獻。

包含三個組件：

(1) Explanation Processor

將結構化的數值解釋（class name、probability、feature name、importance value）：

重排序（由強到弱）

以 placeholder（F1, F2,… C1, C2…）取代真實名稱

線性化成固定格式字串（linearization）

例如：

Predicted class is C1, value of 94.37%.
Top features are [F4, F6, F10 ...].
Positive features are [...]
Negative features are [...]

(2) Textual Explanation Generator

使用 BART / T5

對上述處理後的字串做 sequence-to-sequence 生成

目標：產生專家風格的自然語言敘述

(3) Post-Processor

將 placeholder 替換回原始特徵名稱與類別名稱

輸出完整可讀的自然語言解釋

4. 資料集（TEXEN）

TEXEN 是本研究建立的新資料集，由：

40 個資料集

10 種模型

4 種 XAI 技術（SHAP、LIME、IG、LRP）

496 筆「數值解釋 → 人工撰寫文字解釋」配對

人工撰寫的敘述由 8 位電腦科學專家提供，包含：

模型預測與機率

主要正向貢獻特徵

主要負向貢獻特徵

次要影響因素

研究並提出 TEXEN-Augmented（10 倍增強版本），改善模型泛化性。

5. 實驗與結果
5.1 文本生成（Textual Explanation Generation）

使用 BLEU、METEOR、BLEURT 評估：

BART / T5 模型皆能生成 流暢且結構完整 的文字解釋

資料增強（Augmented）提高語句正確率

Top-n=10 的設定通常比 Top-n=20 表現更佳

Summary 類句子容易產生語意錯誤，需要更多訓練資料

5.2 問答任務（Question Answering）

研究額外建立一個問答資料集，用於分析模型是否真正理解特徵重要度：

T5 在整體問答任務中達到 91% accuracy

單值查詢（如「F5 的值是什麼？」）準確率最高

涉及列表的問題（如「哪些特徵是正向？」）較困難

6. 研究貢獻

提出新的資料集 TEXEN：可用於「特徵重要度 → 自然語言」任務

建立 Textual Explanation Pipeline：可套用於任何 XAI 技術

驗證 BART / T5 能生成自然語言的 XAI 說明

提供資料增強方法提升 NLG 效果

首次以神經網路方式執行「XAI 數值解釋 → 自然語言敘述」

7. 研究限制與未來方向
限制

敘述資料量有限（專家標記成本高）

某些句型（特別是群組比較）容易出錯

僅限於 tabular 型解釋

未來方向

利用 multi-modal（如 image captioning）直接讀取 XAI 圖表

擴充資料集

設計更通用的語義模板與敘述格式

與大型語言模型整合，提升語義精度

8. 總結

本研究成功展示：

如何用自然語言生成技術（NLG）將模型特徵重要度轉換成可讀的敘述。

此流程能有效補足目前 SHAP／LIME 等方法的可解釋性弱點，使使用者更容易理解模型判斷邏輯。

9. 可用於本系統（L1 Semantic Layer）的參考重點
本篇文獻符合

數值解釋 (SHAP) → 自然語言摘要 → 提供 L2 LLM 作為推理輸入


本研究提供的方法可直接用於：

建立「語義模板（semantic templates）」

將 SHAP 轉成固定格式文字

產生 L2 所需的敘述上下文

提供“特徵正向/負向/中性”標籤的語言化規則
