文獻導讀
Ergün, M. (2023). Explaining XGBoost predictions with SHAP value: A comprehensive guide to model interpretation.
https://journals.vilniustech.lt/index.php/NTCS/article/view/17901

1. 研究背景與動機

XGBoost 及其他樹模型在分類與預測任務中廣泛應用，但其內部決策過程不直覺，屬於典型黑盒模型。
傳統特徵重要度（如 gain、cover、weight）存在以下問題：

容易被高基數特徵（high-cardinality）誤導

特徵重要度無法反映方向與局部影響

無法解釋單一預測的成因

為了解決模型可解釋性不足的問題，本研究探討 SHAP values 是否能對 XGBoost 產生透明且一致的解釋。

2. 研究目的

本研究的主要目標為：

建立 XGBoost 結合 SHAP 的完整解釋方法

比較 SHAP 與 XGBoost 內建重要度的差異

使用 SHAP 提供 local 與 global 兩種層級的可解釋性

透過實驗展示 SHAP 如何改善模型透明度與模型行為理解

評估 SHAP 在特徵不同 resolution（粒度設定）下的穩定性

3. 研究方法與架構
3.1 Shapley Value 公式基礎

基於合作博弈論中 marginal contribution 的概念，
透過所有特徵子集的加入／移除，估計每個特徵對最終預測的貢獻。

Shapley values 滿足：

Local accuracy

Consistency

Missingness

使其成為可解釋模型的重要基礎。

3.2 SHAP 與 Tree Models

本研究採用 TreeSHAP（model-specific 方法）：

適用於 XGBoost / LightGBM / Random Forest

能大幅降低 Shapley values 的計算成本

提供 fast & exact 解釋（相較於 model-agnostic SHAP）

能兼容 boosted trees 的結構

3.3 Local Explainability

利用 SHAP 分解單一預測：

顯示 baseline → final prediction 的累進過程

標示每個特徵的正向／負向貢獻

用 force plot、waterfall plot 做視覺化

能用於模型 debugging 與異常判定

3.4 Global Explainability

SHAP 可建立全模型層級的重要度分析：

Mean(|SHAP|) 作為特徵重要度

Summary Plot（Beeswarm）呈現影響方向與分佈

特徵 resolution 敏感性測試

SHAP Dependence Plot 分析非線性關係

本研究強調 SHAP 能避免 XGBoost 內建 importance 遭特徵偏誤影響的問題。

4. 實驗設計

包含兩組重點實驗：

4.1 實驗 1：不同 resolution 之特徵重要度比較

比較：

XGBoost 內建 gain importance

SHAP global importance

結果顯示：

XGBoost importance 對特徵分箱（resolution）非常敏感

SHAP importance 具穩定性，不受特徵 scaling 或分箱策略影響

4.2 實驗 2：Local explanation 展示

針對單一資料點：

XGBoost importance 無法解釋其預測原因

SHAP 可呈現正向／負向因素

具體展示模型行為與特徵相互作用

結論指出 SHAP 在單點推論的診斷能力上顯著優於傳統方法。

5. 研究結果

研究實證結果如下：

SHAP 在 global importance 上比 XGBoost 內建方法更穩定可靠

SHAP 能提供 XGBoost 缺乏的 local explanation，揭露單一預測的構成

特徵 resolution 測試顯示 XGBoost importance 容易偏誤，而 SHAP 不受影響

SHAP summary plot 能同時顯示影響方向（正／負）與影響程度

研究提出 SHAP 更適合用於敏感、可審計或高風險領域模型

6. 研究貢獻

建立 XGBoost + SHAP 的系統化模型解釋流程

提出 resolution-sensitive 實驗，展示 SHAP 的穩健性

完整比較 SHAP 與 XGBoost 內建重要度指標

示範 SHAP local explanation 在 debugging 與模型監控的價值

提供提升樹模型透明度的具體方法

7. 研究限制與未來方向
限制

僅以 tabular data（表格資料）為主

未涵蓋時序特徵的 SHAP 擴充方法

部分 SHAP 圖形對非技術使用者仍較難理解

未來方向

與自然語言生成（NLG）結合，產生語義化解釋

擴展至 sequence-aware SHAP（用於時序金融資料）

與模型監控框架整合，用於 drift / regime change 偵測

8. 總結

本研究展示 SHAP 在解釋 XGBoost 預測中的優勢，包括：

全域與局部解釋能力

避免特徵分箱與尺度偏誤

深度揭露模型邏輯

提供可審計、可監控的透明模型結果

因此 SHAP 是樹模型應用中最合適的解釋技術之一。

9. Applicability to This System（可用於本系統的參考重點）
9.1 L0 → L1 的主要學術支撐

本研究直接針對 XGBoost + SHAP，可作為 L1 模組的主要引用。

9.2 支援 L1 語義摘要模板

SHAP 的正負貢獻能直接轉換為 L1 的語義格式，例如：

RSI (+0.37) → 推升多頭概率  
Volume (-0.22) → 壓低價格動能  
MA Spread (+0.12) → 中度偏多訊號  

9.3 支援 L1 → L2 之決策資訊流

L1 可利用 SHAP 提供：

特徵貢獻排序

影響方向

非線性行為摘要

這些資訊能直接作為 L2（LLM Decision Agent）的推理輸入。

9.4 支援論文中「可解釋性需求」的論述

研究強調 SHAP 比 XGBoost 原生 importance 更可靠，可用於：

交易風控

模型審計

異常偵測

模型失效分析（model breakdown）