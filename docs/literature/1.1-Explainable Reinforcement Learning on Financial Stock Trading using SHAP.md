文獻導讀：Kumar (2022)  https://arxiv.org/pdf/2208.08790
Explainable Reinforcement Learning for Financial Trading
1. 研究背景與動機

金融交易領域越來越多採用 ML 與 RL 建構策略，但由於市場受到監管、風險控管要求高，模型「可解釋性」成為必要條件。傳統 RL 模型沒有辦法提供行為原因，使其難以部署。

此研究提出使用 SHAP 來解釋 RL 交易模型的決策依據，使模型行為能被理解與審核。

2. 研究目的

建立能解釋交易決策的 RL 模型

使用 SHAP 值定量各特徵對買/賣/觀望決策的影響

提升交易代理人在金融環境中的透明度

3. 研究方法與架構
3.1 交易代理模型

使用 PPO（Proximal Policy Optimization）作為 RL backbone

輸入特徵包含多種技術指標

3.2 SHAP 可解釋性分析

使用 TreeSHAP 量化每次行為的貢獻度

輸出每個動作的「特徵重要度列表」

區分正向影響與負向影響

4. 實作與資料

使用股價日資料建立訓練集與測試集

評估代理人在不同市場 regime（震盪、上升、下降）下的 SHAP 行為

比較 SHAP 分析後的行為一致性與透明度

5. 研究結果

SHAP 能清楚揭示模型在何時依賴 RSI、MACD、價格動量等特徵

特徵重要度因市場狀態不同而改變

解釋性提升後，策略行為更容易被風控部門接受

SHAP 有助於找出模型過度依賴的特徵

6. 研究貢獻

建立 SHAP 用於交易模型的流程

將 ML 模型從「黑盒」轉為「可觀察決策邏輯」

強化模型透明度，有助部署與審核

提供 feature contribution 的量化方式

7. 研究限制與未來方向

限制：

僅使用日資料，不含高頻微結構特徵

僅針對 RL，不包含 boosting 類模型

SHAP 計算成本較高

未來方向：

與 boosting 模型整合

自動化 SHAP 轉語義敘述

引入更高頻市場資料

8. 總結

本研究展示 SHAP 在金融交易模型可解釋性上的實務價值。研究指出，透明化的交易訊號解釋能協助策略審核、降低風險、提高模型部署可信度。

9. Applicability to This System（可用於本系統的參考重點）
9.1 對應 L1 層核心：SHAP → 語義轉譯

研究提供「特徵貢獻度 → 解釋模型行為」的標準流程，是 L1 層語義生成（Feature-to-Text）的直接先例。

9.2 顯示解釋性是金融 ML 模型的必要條件

可用於支持 L1 層的研究動機。

9.3 特徵重要度方向性（positive/negative）可直接用於語義生成模板

例如：

SHAP > 0 → 上漲因素

SHAP < 0 → 下跌因素

9.4 支援構建「模型決策摘要」邏輯

如：

「模型因 RSI 下行 + Volume 增加而判定為 SELL」
這正是 L1 層輸出的語義描述格式。

9.5 可直接引用為本系統 L1 設計的理論基礎